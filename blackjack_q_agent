import gymnasium as gym
import numpy as np 
from collections import defaultdict
import matplotlib.pyplot as plt
import torch
import torch.nn as nn
import torch.optim as optim
import random
from collections import deque, namedtuple




q_table = defaultdict(lambda: np.zeros(2))

learning_rate = 0.01
gamma = 0.95
epsilon = 1.0
epsilon_decay = 0.9995
min_epsilon = 0.05

def boltzmann_action(state, temperature):
    q_values = q_table[state]
    # Subtracting max for numerical stability (prevents overflow)
    exp_q = np.exp((q_values - np.max(q_values)) / temperature)
    probs = exp_q / np.sum(exp_q)
    return np.random.choice([0, 1], p=probs)

env = gym.make("Blackjack-v1")
obs, info = env.reset()
done = False

def choose_action(state, epsilon):
    if np.random.random() < epsilon:
        return env.action_space.sample() #Exploration
    else:
        return np.argmax(q_table[state]) #Exploitation
    
num_episodes = 100_000
rewards_history = []

def train_blackjack(episodes, strategy="epsilon", param=1.0, decay=0.9995, min_val=0.05):
    # Reset Q-table for a fresh start
    local_q_table = defaultdict(lambda: np.zeros(2))
    history = []
    current_val = param # This is epsilon or temperature

    for episode in range(episodes):
        obs, _ = env.reset()
        done = False
        total_reward = 0
        
        while not done:
            # Switch between strategies
            if strategy == "epsilon":
                if np.random.random() < current_val:
                    action = env.action_space.sample()
                else:
                    action = np.argmax(local_q_table[obs])
            else: # Boltzmann
                q_values = local_q_table[obs]
                exp_q = np.exp((q_values - np.max(q_values)) / current_val)
                probs = exp_q / np.sum(exp_q)
                action = np.random.choice([0, 1], p=probs)

            next_obs, reward, terminated, truncated, _ = env.step(action)
            done = terminated or truncated
            
            # Q-learning update
            next_q_max = 0 if done else np.max(local_q_table[next_obs])
            td_target = reward + gamma * next_q_max
            local_q_table[obs][action] += learning_rate * (td_target - local_q_table[obs][action])
            
            obs = next_obs
            total_reward += reward

        # Decay logic
        current_val = max(min_val, current_val * decay)
        history.append(total_reward)
    
    return history

# Run both for comparison (this takes a moment to run)
print("Training Epsilon-Greedy...")
eps_history = train_blackjack(100000, strategy="epsilon")

print("Training Boltzmann...")
bolt_history = train_blackjack(100000, strategy="boltzmann", param=1.0, decay=0.9999)

# Revised plotting function to compare both
def plot_comparison(eps_rew, bolt_rew, window=500):
    plt.figure(figsize=(10, 5))

    if len(eps_rew) >= window:
        eps_avg = np.convolve(eps_rew, np.ones(window)/window, mode='valid')
        plt.plot(eps_avg, label="Epsilon-Greedy")

    if len(bolt_rew) >= window:
        bolt_avg = np.convolve(bolt_rew, np.ones(window)/window, mode='valid')
        plt.plot(bolt_avg, label="Boltzmann (Softmax)")

    plt.axhline(y=-0.15, color='r', linestyle='--', label="Random Policy Baseline")
    
    plt.title("Blackjack: Epsilon-Greedy vs Boltzmann Exploration")
    plt.xlabel("Episode")
    plt.ylabel(f"Moving Average Reward (window={window})")
    plt.legend()
    plt.grid(True, alpha=0.3)
    plt.show()

# Call the new comparison plot
plot_comparison(eps_history, bolt_history)










# import gymnasium as gym
# import torch
# import torch.nn as nn
# import torch.optim as optim
# import numpy as np
# import random
# from collections import deque, namedtuple
# import matplotlib.pyplot as plt

# # --- Hyperparameters ---
# BATCH_SIZE = 128
# GAMMA = 0.99
# EPS_START = 0.9
# EPS_END = 0.05
# EPS_DECAY = 1000 # Controls the speed of epsilon decay
# TAU = 0.005      # Soft update rate
# LR = 1e-4
# MEMORY_SIZE = 10000

# # Transition tuple to store experiences
# Transition = namedtuple('Transition', ('state', 'action', 'next_state', 'reward', 'done'))

# # --- 1. The Neural Network ---
# class DQN(nn.Module):
#     def __init__(self, n_observations, n_actions):
#         super(DQN, self).__init__()
#         self.layer1 = nn.Linear(n_observations, 128)
#         self.layer2 = nn.Linear(128, 128)
#         self.layer3 = nn.Linear(128, n_actions)

#     def forward(self, x):
#         x = torch.relu(self.layer1(x))
#         x = torch.relu(self.layer2(x))
#         return self.layer3(x)

# # --- 2. The Replay Memory ---
# class ReplayMemory:
#     def __init__(self, capacity):
#         self.memory = deque([], maxlen=capacity)

#     def push(self, *args):
#         self.memory.append(Transition(*args))

#     def sample(self, batch_size):
#         return random.sample(self.memory, batch_size)

#     def __len__(self):
#         return len(self.memory)

# # --- 3. The Agent Setup ---
# env = gym.make("LunarLander-v3")
# n_actions = env.action_space.n
# n_observations = env.observation_space.shape[0]

# policy_net = DQN(n_observations, n_actions)
# target_net = DQN(n_observations, n_actions)
# target_net.load_state_dict(policy_net.state_dict())

# optimizer = optim.AdamW(policy_net.parameters(), lr=LR, amsgrad=True)
# memory = ReplayMemory(MEMORY_SIZE)

# steps_done = 0

# def select_action(state):
#     global steps_done
#     sample = random.random()
#     eps_threshold = EPS_END + (EPS_START - EPS_END) * np.exp(-1. * steps_done / EPS_DECAY)
#     steps_done += 1
#     if sample > eps_threshold:
#         with torch.no_grad():
#             return policy_net(state).max(1)[1].view(1, 1)
#     else:
#         return torch.tensor([[env.action_space.sample()]], dtype=torch.long)

# # --- 4. The Training Logic ---
# def optimize_model():
#     if len(memory) < BATCH_SIZE:
#         return

#     transitions = memory.sample(BATCH_SIZE)
#     batch = Transition(*zip(*transitions))

#     state_batch = torch.cat(batch.state)
#     action_batch = torch.cat(batch.action)
#     reward_batch = torch.cat(batch.reward)
#     next_state_batch = torch.cat(batch.next_state)
#     done_batch = torch.cat(batch.done)

#     # Current Q values
#     state_action_values = policy_net(state_batch).gather(1, action_batch)

#     # Expected Q values (using Target Net)
#     with torch.no_grad():
#         next_state_values = target_net(next_state_batch).max(1)[0]
    
#     expected_state_action_values = (next_state_values * GAMMA * (1 - done_batch)) + reward_batch

#     # Huber Loss
#     criterion = nn.SmoothL1Loss()
#     loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))

#     optimizer.zero_grad()
#     loss.backward()
#     optimizer.step()

# # --- 5. Main Loop ---
# num_episodes = 500
# reward_history = []

# for i_episode in range(num_episodes):
#     # Every 50 episodes, render the game to see progress
#     if i_episode % 50 == 0 and i_episode > 0:
#         render_env = gym.make("LunarLander-v3", render_mode="human")
#         state, _ = render_env.reset()
#     else:
#         state, _ = env.reset()
    
#     state = torch.tensor(state, dtype=torch.float32).unsqueeze(0)
#     total_reward = 0
    
#     for t in range(1000): # Limit steps per episode
#         action = select_action(state)
        
#         # Use render_env if we are in a visualization episode
#         active_env = render_env if (i_episode % 50 == 0 and i_episode > 0) else env
#         observation, reward, terminated, truncated, _ = active_env.step(action.item())
        
#         done = terminated or truncated
#         total_reward += reward
#         reward_tensor = torch.tensor([reward], dtype=torch.float32)
#         done_tensor = torch.tensor([float(done)], dtype=torch.float32)

#         next_state = torch.tensor(observation, dtype=torch.float32).unsqueeze



# def plot_lunar_results(rewards):
#     plt.figure(figsize=(10, 6))
    
#     # Plot the raw rewards as faint dots/lines
#     plt.plot(rewards, alpha=0.3, color='blue', label='Episode Reward')
    
#     # Calculate and plot a moving average (window of 25 episodes)
#     if len(rewards) >= 25:
#         moving_avg = np.convolve(rewards, np.ones(25)/25, mode='valid')
#         plt.plot(range(24, len(rewards)), moving_avg, color='red', linewidth=2, label='Moving Average (25)')
    
#     # Add a horizontal line at 200 (The "Solved" threshold for Lunar Lander)
#     plt.axhline(y=200, color='green', linestyle='--', label='Solved Threshold')
    
#     plt.title('DQN Lunar Lander Training Progress')
#     plt.xlabel('Episode')
#     plt.ylabel('Total Reward')
#     plt.legend()
#     plt.grid(True, alpha=0.3)
#     plt.tight_layout()
#     plt.show()

# # Use the history collected in your main loop
# plot_lunar_results(reward_history)